# Story 1.20: Optimizaci√≥n del Rendimiento de Subida

## Status

Draft

## Story

**As a** System Operator,
**I want** the artifact upload pipeline to be highly optimized for performance,
**so that** the system can handle a high throughput of concurrent uploads with minimal latency and resource consumption.

## Acceptance Criteria

1. The upload process must leverage high-performance I/O patterns (e.g., `tokio::io`).
2. Memory management should be optimized by using object pools for buffers to reduce allocations.
3. Content-defined chunking (e.g., Rabin fingerprinting) should be used for more effective deduplication, especially for similar but not identical files.
4. For deduplication checks, a Bloom filter or similar probabilistic data structure should be used to avoid expensive database lookups for unique files.
5. The entire pipeline, from request reception to event publication, must be fully asynchronous and non-blocking.

## Tasks / Subtasks

- [ ] Refactor the upload handler to use an object pool for I/O buffers.
- [ ] Implement a Rabin fingerprinting algorithm for content-defined chunking.
- [ ] Integrate a Bloom filter for deduplication checks.
- [ ] Profile and benchmark the entire upload pipeline to identify and eliminate bottlenecks.
- [ ] Optimize database queries related to artifact creation.
- [ ] Write performance tests to validate throughput and latency improvements.

## Dev Notes

### Technical Constraints
- These optimizations add complexity and should be undertaken after the core functionality is stable.
- The choice of chunking and filtering algorithms should be benchmarked to find the best trade-off between performance and effectiveness.