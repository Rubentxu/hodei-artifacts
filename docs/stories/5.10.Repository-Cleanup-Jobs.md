# Story 5.10: Repository Cleanup Jobs

## Status
- Draft

## Story
**Como** un Administrador del Sistema,
**quiero** ejecutar un trabajo de limpieza de almacenamiento (recolección de basura),
**para** purgar de forma segura los datos huérfanos y reclamar el espacio en disco que ha sido liberado lógicamente por la eliminación de artefactos.

## Acceptance Criteria
1.  Un administrador puede iniciar un trabajo de recolección de basura para todo el sistema (o para un almacenamiento específico) a través de la API.
2.  El trabajo debe identificar correctamente todos los blobs de datos en el almacenamiento que no están referenciados por ningún metadato de artefacto activo.
3.  El trabajo debe eliminar de forma permanente y segura únicamente los blobs de datos huérfanos.
4.  El trabajo NUNCA debe eliminar blobs de datos que todavía están siendo referenciados por artefactos válidos.
5.  Se puede consultar el estado de un trabajo de limpieza en curso o del último ejecutado (ej: `RUNNING`, `COMPLETED`, `FAILED`), incluyendo estadísticas como el espacio recuperado.
6.  Solo una instancia del trabajo de limpieza puede ejecutarse a la vez en todo el sistema para evitar conflictos.

## Tasks / Subtasks
- [ ] Tarea 1: Diseñar el modelo de datos para el seguimiento de los trabajos de limpieza. (AC: 5)
    - [ ] Subtarea 1.1: Crear una nueva colección en MongoDB `cleanup_jobs` para almacenar el estado, progreso, estadísticas y resultado de cada ejecución.
- [ ] Tarea 2: Implementar el servicio de recolección de basura ("Garbage Collector"). (AC: 2, 3, 4)
    - [ ] Subtarea 2.1: Implementar la fase "Mark": crear una función que recorra la base de datos de metadatos y construya un `HashSet` con los identificadores (ej: checksums SHA256) de todos los blobs de datos que están actualmente en uso.
    - [ ] Subtarea 2.2: Implementar la fase "Sweep": crear una función que liste todos los objetos en el backend de almacenamiento (ej: bucket S3) y compare cada objeto con el `HashSet` de la fase "Mark".
    - [ ] Subtarea 2.3: Si un objeto del almacenamiento no está en el `HashSet`, se añade a una lista de "candidatos a eliminar".
    - [ ] Subtarea 2.4: Implementar la lógica de eliminación, que procesará la lista de candidatos y los borrará del almacenamiento, actualizando las estadísticas del job.
- [ ] Tarea 3: Exponer la funcionalidad a través de la API y el sistema de trabajos. (AC: 1, 5, 6)
    - [ ] Subtarea 3.1: Crear un endpoint `POST /api/v1/system/cleanup-jobs` que inicie un nuevo trabajo de limpieza (si no hay uno ya en ejecución) y devuelva el ID del job.
    - [ ] Subtarea 3.2: Crear un endpoint `GET /api/v1/system/cleanup-jobs/{job_id}` para consultar el estado del trabajo.
    - [ ] Subtarea 3.3: Integrar la ejecución del `GarbageCollectorService` con el sistema de trabajos en segundo plano, permitiendo la ejecución manual (vía API) y programada (ej: semanalmente).
- [ ] Tarea 4: Pruebas de seguridad y robustez.
    - [ ] Subtarea 4.1: Crear un escenario de prueba que simule un repositorio con artefactos activos y huérfanos.
    - [ ] Subtarea 4.2: Ejecutar el trabajo de limpieza en el escenario de prueba y verificar que SOLAMENTE los datos huérfanos fueron eliminados.
    - [ ] Subtarea 4.3: Probar la resiliencia del trabajo: simular un fallo a mitad del proceso y verificar que no se hayan perdido datos válidos al reanudar o reiniciar.

## Dev Notes

### Relevant Source Tree info
- **ASUNCIÓN**: Esta lógica, al ser a nivel de sistema, podría residir en un nuevo crate o en un módulo de alto nivel como `crates/system/src/cleanup/`.

### API Specifications
- **ASUNCIÓN**:
    - `POST /api/v1/system/cleanup-jobs`: Inicia la recolección de basura. Devuelve `202 Accepted` con el `job_id`.
    - `GET /api/v1/system/cleanup-jobs/latest`: Devuelve el estado del último job ejecutado o en curso.

### Architecture Considerations
- **Decisión Arquitectónica Clave**: Se utilizará el algoritmo **Mark and Sweep**. Este enfoque es robusto y estándar en la industria para la recolección de basura.
    - **Fase 1 (Mark)**: Es una operación intensiva en lectura de la base de datos. Se debe optimizar para escanear los metadatos de la forma más eficiente posible.
    - **Fase 2 (Sweep)**: Es una operación intensiva en I/O con el sistema de almacenamiento. Es la fase más larga y peligrosa. Se debe implementar con sumo cuidado, incluyendo logging exhaustivo y, potencialmente, un modo "dry run" (simulación) que solo reporte lo que se eliminaría, sin borrarlo realmente.
- **Impacto en el Rendimiento**: Este trabajo puede degradar el rendimiento del almacenamiento y la base de datos mientras se ejecuta. Debe programarse para horas de baja actividad (noches, fines de semana) y su concurrencia debe ser estrictamente controlada (singleton).

### Testing
- **ASUNCIÓN**: La prevención de la pérdida de datos es la máxima prioridad. Las pruebas deben ser extremadamente rigurosas. Es recomendable que la lógica de eliminación, en lugar de borrar inmediatamente, mueva primero los objetos a una ubicación de "cuarentena" con un TTL (Time-To-Live) corto. Si no se reportan problemas después de un tiempo, un segundo proceso más simple puede purgar definitivamente la cuarentena.