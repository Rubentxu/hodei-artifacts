# Story 1.13: Eventos de Anal√≠tica de Subida

## Status

Draft

## Story

**As a** Data Analyst or System Administrator,
**I want** the system to emit detailed telemetry events for every stage of the upload process,
**so that** I can build dashboards and alerts to monitor the health, performance, and usage patterns of the upload service.

## Acceptance Criteria

1. The system must emit discrete events for key stages: `ArtifactUploadStarted`, `ArtifactHashCalculated`, `ArtifactValidationFailed`, `DuplicateArtifactDetected`, `ArtifactUploaded`, etc.
2. Events must be structured (e.g., JSON) and contain rich contextual information (user ID, repository ID, file size, duration, etc.).
3. These events should be published to a dedicated analytics stream in Kafka or another messaging system.
4. The event schemas must be versioned and documented.
5. The emission of analytics events should have minimal performance impact on the upload path.

## Tasks / Subtasks

- [ ] Define the schema for all detailed upload-related analytics events.
- [ ] Instrument the upload code to emit these events at the appropriate stages.
- [ ] Configure an event publisher to send these events to the analytics stream.
- [ ] Ensure all relevant context (timing, sizes, IDs) is included in the events.
- [ ] Write tests to ensure events are emitted correctly for all upload scenarios (success, failure, duplicate).

## Dev Notes

### Data Models
- This introduces new event models, which should be defined and versioned carefully.
- These are distinct from the core domain events and are intended for a different consumer (analytics vs. application logic).