# Story 1.2: Subida de Artefactos Multi-parte

## Status

Draft

## Story

**As a** CI/CD System or Developer,
**I want** to upload large artifacts (>100MB) using multipart streaming,
**so that** the upload process is memory-efficient and resilient to network interruptions.

## Acceptance Criteria

1. The system must support multipart/form-data streaming for artifact uploads.
2. The upload process must not load the entire file into memory at once.
3. The system must be able to handle files larger than available RAM.
4. Checksums must be calculated on the fly as chunks are received.
5. The system should provide a mechanism to resume interrupted uploads (corresponds to feature E1.F08).
6. The API should gracefully handle network errors during the stream.
7. An `UploadProgressUpdated` event could be published periodically.
8. The final `ArtifactUploaded` event is only published after all parts are successfully received and assembled.

## Tasks / Subtasks

- [ ] Enhance the Axum endpoint to handle multipart streams efficiently.
- [ ] Implement chunk-by-chunk processing of the uploaded file.
- [ ] Integrate streaming hash calculation (SHA-256).
- [ ] Implement logic for temporary storage of parts for resumable uploads.
- [ ] Develop robust error handling for stream interruptions.
- [ ] Write unit and integration tests for the multipart upload flow.
- [ ] Test with very large files to ensure memory efficiency.

## Dev Notes

### Data Models
- No new data models. This story enhances the existing `Artifact` and `PhysicalArtifact` lifecycle.

### API Specifications
- **Endpoint**: `POST /artifacts` (enhanced)
- The endpoint will now process the `file` part of the multipart request as a stream.
- May require additional headers for managing resumable uploads (e.g., `Upload-ID`, `Chunk-Number`).

### Technical Constraints
- **Memory Usage**: Must remain low and constant regardless of file size.
- **Performance**: Streaming should be implemented with minimal overhead.