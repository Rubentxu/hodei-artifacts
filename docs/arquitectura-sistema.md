# Arquitectura del Sistema: Hodei Artifacts

**Versi√≥n**: 1.2 (Revisada con Gu√≠a Pr√°ctica)
**Fecha**: 24 de Agosto, 2025
**Autor**: Solution Architect & Tech Lead
**Estado**: Documento Vivo

## üéØ Resumen Ejecutivo

Hodei Artifacts es un repositorio de artefactos de nueva generaci√≥n dise√±ado para ofrecer m√°ximo rendimiento, seguridad y escalabilidad. Construido en **Rust**, el sistema adopta una arquitectura h√≠brida que fusiona **Vertical Slice Architecture (VSA)**, **Arquitectura Hexagonal** y un modelo **Dirigido por Eventos (EDA)**.

Nuestra estrategia es un **monolito modular evolutivo**: un enfoque pragm√°tico que nos otorga la velocidad de desarrollo de un sistema unificado al inicio, mientras que la estricta modularidad de los *Slices* verticales nos garantiza la flexibilidad para extraer funcionalidades a microservicios en el futuro sin reescrituras masivas.

### Decisiones Arquitect√≥nicas Clave

* **Lenguaje**: Rust, por su rendimiento, seguridad de memoria y concurrencia de primer nivel.
* **Patr√≥n Arquitect√≥nico**: H√≠brido VSA + Hexagonal + EDA para combinar cohesi√≥n funcional, desacoplamiento y escalabilidad as√≠ncrona.
* **Estrategia Evolutiva**: Iniciar como monolito modular para una r√°pida puesta en marcha, con un camino claro hacia la extracci√≥n de microservicios.
* **Almacenamiento**: Binarios en un backend compatible con **S3** (ej. MinIO) para escalabilidad infinita; metadatos en **MongoDB** por su flexibilidad y capacidad de indexaci√≥n.
* **Comunicaci√≥n**: **API REST** para interacciones s√≠ncronas cliente-servidor y **Apache Kafka** para la comunicaci√≥n as√≠ncrona y desacoplada entre los diferentes m√≥dulos del sistema.
* **Autorizaci√≥n**: Modelo de Control de Acceso Basado en Atributos (**ABAC**) con el motor de pol√≠ticas **Cedar** para una gesti√≥n de permisos granular y flexible.

## üìê Principios Arquitect√≥nicos Fundamentales

1. **Alta Cohesi√≥n Funcional (Vertical Slices)**: Organizamos el c√≥digo en torno a capacidades de negocio completas ("slices"), no por capas t√©cnicas. Esto minimiza el acoplamiento entre funcionalidades y facilita el trabajo de equipos aut√≥nomos.
2. **Inversi√≥n de Dependencias (Arquitectura Hexagonal)**: El n√∫cleo de la l√≥gica de negocio (dominio) es agn√≥stico a la infraestructura. Define "puertos" (interfaces) que son implementados por "adaptadores" (bases de datos, APIs externas), asegurando que el dominio permanezca puro y f√°cilmente testeable.
3. **Comunicaci√≥n As√≠ncrona por Defecto**: La comunicaci√≥n entre los *Slices* se realiza preferentemente a trav√©s de eventos. Esto aumenta la resiliencia, el desacoplamiento y la escalabilidad del sistema.
4. **Contratos de API Primero (Contract-First)**: La especificaci√≥n **OpenAPI** es la fuente de verdad para nuestras APIs. Esto permite el desarrollo en paralelo del frontend y backend y garantiza la coherencia de la interfaz.

## üèóÔ∏è Vista de Alto Nivel

### Arquitectura L√≥gica

El sistema est√° compuesto por Slices Verticales cohesivos que exponen su funcionalidad a trav√©s de una API Gateway. Estos Slices se comunican entre s√≠ de forma as√≠ncrona mediante un Bus de Eventos y persisten sus datos en una capa de almacenamiento compartida pero l√≥gicamente separada.

```mermaid
graph TB
    subgraph "Usuarios y Sistemas Externos"
        CI[Sistemas CI/CD]
        DEV[Desarrolladores]
        SEC[Herramientas de Seguridad]
    end

    subgraph "Plataforma Hodei Artifacts"
        subgraph "Capa de Acceso"
            API[API Gateway]
        end

        subgraph "M√≥dulos de Negocio - Vertical Slices"
            VS1[Ingesta de Artefactos]
            VS2[Recuperaci√≥n de Artefactos]
            VS3[B√∫squeda y Descubrimiento]
            VS4[Escaneo de Seguridad]
            VS5[Gesti√≥n de Repositorios]
            VS6[Gesti√≥n de Usuarios y Pol√≠ticas]
        end

        subgraph "Infraestructura Central"
            KAFKA[Bus de Eventos Kafka]
            MONGO[Metadata DB - MongoDB]
            S3[Object Storage - S3]
            CACHE[Cache - Redis]
        end
    end

    %% Flujo de Interacciones
    CI & DEV & SEC --> API

    API -- Enforce Auth --> VS1 & VS2 & VS3 & VS4 & VS5 & VS6

    %% Comunicaci√≥n As√≠ncrona
    VS1 -- Publica Eventos --> KAFKA
    VS4 -- Publica Eventos --> KAFKA
    VS5 -- Publica Eventos --> KAFKA

    KAFKA -- Consume Eventos --> VS3
    KAFKA -- Consume Eventos --> VS4

    %% Persistencia
    VS1 & VS2 & VS3 & VS4 & VS5 & VS6 --> MONGO
    VS1 & VS2 --> S3
    API -- Decisiones de Auth cacheadas --> CACHE
```

* *Nota sobre el diagrama: La API Gateway act√∫a como **Policy Enforcement Point (PEP)**, validando cada petici√≥n antes de dirigirla al Slice correspondiente.*

## üîß Stack Tecnol√≥gico


| Categor√≠a         | Componente                | Tecnolog√≠a                   | Justificaci√≥n                                                                                               |
| :----------------- | :------------------------ | :---------------------------- | :----------------------------------------------------------------------------------------------------------- |
| **Core Backend**   | Lenguaje                  | **Rust**                      | Rendimiento, seguridad de memoria y concurrencia.                                                            |
|                    | Runtime As√≠ncrono        | **Tokio**                     | Ecosistema maduro y est√°ndar de facto para I/O as√≠ncrono en Rust.                                          |
|                    | Web Framework             | **Axum**                      | Ergon√≥mico, alto rendimiento y perfecta integraci√≥n con el ecosistema Tokio.                               |
|                    | Serializaci√≥n            | **Serde**                     | Framework de serializaci√≥n/deserializaci√≥n universal y de alto rendimiento.                                |
| **Persistencia**   | Base de Datos (Metadata)  | **MongoDB**                   | Flexibilidad de esquema para metadatos ricos y potentes capacidades de indexaci√≥n.                          |
|                    | Almacenamiento de Objetos | **Compatible con S3**         | Interfaz est√°ndar para almacenamiento de binarios escalable y desacoplado del proveedor.                    |
|                    | Cach√©                    | **Redis**                     | Almacenamiento en memoria de baja latencia para sesiones, decisiones de autorizaci√≥n y metadatos calientes. |
| **Mensajer√≠a**    | Bus de Eventos            | **Apache Kafka**              | Plataforma de streaming de eventos distribuida, duradera y de alto rendimiento.                              |
|                    | Registro de Esquemas      | **Confluent Schema Registry** | Gobierna la evoluci√≥n de los esquemas de eventos, evitando breaking changes.                                |
| **Seguridad**      | Motor de Pol√≠ticas       | **Cedar**                     | Lenguaje de pol√≠ticas declarativo para control de acceso granular (ABAC).                                   |
|                    | Tokens                    | **JWT (jsonwebtoken)**        | Est√°ndar para la validaci√≥n de tokens de autenticaci√≥n.                                                   |
| **Observabilidad** | M√©tricas                 | **Prometheus**                | Est√°ndar de la industria para la recolecci√≥n de m√©tricas.                                                 |
|                    | Trazas y Logs             | **Tracing + OpenTelemetry**   | Para logging estructurado y trazado distribuido de peticiones.                                               |
| **Tooling**        | Contenerizaci√≥n          | **Docker**                    | Empaquetado de la aplicaci√≥n y sus dependencias.                                                            |
|                    | Orquestaci√≥n             | **Kubernetes**                | Despliegue, escalado y gesti√≥n de la aplicaci√≥n containerizada.                                            |

## üé® Arquitectura Funcional: Vertical Slices (VSA) en la Pr√°ctica

La arquitectura VSA es nuestro pilar organizativo. Cada *slice* es un m√≥dulo autocontenido que agrupa toda la l√≥gica para una capacidad de negocio espec√≠fica.

### Organizaci√≥n del C√≥digo: Carpetas por Feature

Para un programador, esto se traduce en una estructura de directorios simple y predecible. Organizamos el c√≥digo por **feature**, no por capa t√©cnica.

```
src/
‚îú‚îÄ‚îÄ features/
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ upload_artifact/        // <-- SLICE VERTICAL 1
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mod.rs              // Define la API p√∫blica del m√≥dulo.
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ handler.rs          // Endpoint de la API (funci√≥n que recibe la petici√≥n HTTP).
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ command.rs          // Definici√≥n del caso de uso (ej. struct UploadArtifactCommand).
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logic.rs            // L√≥gica de negocio pura para este feature.
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ download_artifact/      // <-- SLICE VERTICAL 2
‚îÇ       ‚îú‚îÄ‚îÄ mod.rs
‚îÇ       ‚îú‚îÄ‚îÄ handler.rs
‚îÇ       ‚îú‚îÄ‚îÄ query.rs            // Para lecturas, se suele usar "Query".
‚îÇ       ‚îî‚îÄ‚îÄ logic.rs
‚îÇ
‚îú‚îÄ‚îÄ shared/                     // <-- C√ìDIGO REUTILIZABLE
‚îÇ   ‚îú‚îÄ‚îÄ domain/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ models.rs           // Entidades centrales (ej. struct Artifact).
‚îÇ   ‚îú‚îÄ‚îÄ infrastructure/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database.rs         // Pool de conexiones a BD.
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ storage.rs          // Cliente S3.
‚îÇ   ‚îî‚îÄ‚îÄ web/
‚îÇ       ‚îú‚îÄ‚îÄ middleware.rs       // Middleware de autenticaci√≥n, logging.
‚îÇ       ‚îî‚îÄ‚îÄ errors.rs           // Manejo de errores comunes.
‚îÇ
‚îî‚îÄ‚îÄ main.rs                     // Punto de entrada: solo enruta peticiones a los handlers.
```

### La Regla de Oro de las Dependencias

Este modelo se sostiene sobre una regla fundamental que garantiza el bajo acoplamiento:

> **Un feature NUNCA debe depender directamente de otro feature.**
> **Todos los features PUEDEN depender de `shared`.**
> **El c√≥digo en `shared` NO debe depender de ning√∫n feature.**

```mermaid
graph TD
    FeatureA[Feature: Upload] --> Shared
    FeatureB[Feature: Download] --> Shared
    subgraph "L√≠mite Prohibido"
        FeatureA -- X --- FeatureB
    end
    subgraph "L√≠mite Prohibido"
        Shared -- X --- FeatureA
    end
```

Esto evita el "c√≥digo espagueti" y asegura que cada feature pueda ser modificado (o incluso eliminado) con un impacto m√≠nimo en el resto del sistema.

## üîÄ Patrones de Comunicaci√≥n

*(Esta secci√≥n detalla los flujos de comunicaci√≥n S√≠ncrona (REST) y As√≠ncrona (Eventos) con diagramas de secuencia y ejemplos de payload)*

## üîê Arquitectura de Seguridad (Security by Design)

*(Esta secci√≥n detalla los principios de Zero Trust, Defensa en Profundidad y el modelo de autorizaci√≥n ABAC con ejemplos de pol√≠ticas Cedar)*

## üìä Estrategia de Datos

*(Esta secci√≥n detalla la visi√≥n general del uso de MongoDB, S3, Kafka y Redis, incluyendo un ejemplo del esquema de datos clave en MongoDB)*

## üöÄ Estrategia de Despliegue y Operaci√≥n

*(Esta secci√≥n detalla la Arquitectura de Contenedores con un Dockerfile de ejemplo y la estrategia de despliegue en Kubernetes)*

## üìà Observabilidad

*(Esta secci√≥n detalla los tres pilares (M√©tricas, Logs Estructurados, Trazado Distribuido) con ejemplos)*

## üìö Decisiones Arquitect√≥nicas Registradas (ADRs)

*(Esta secci√≥n lista las decisiones clave y su justificaci√≥n)*

---

## üéì Gu√≠a Pr√°ctica y Escenarios Problem√°ticos

Una arquitectura es tan buena como la disciplina del equipo para mantenerla. A continuaci√≥n, se detallan los problemas m√°s comunes que podemos encontrar y c√≥mo mitigarlos proactivamente.

### 1\. El "Shared Monolith"

* **Problema**: La carpeta `shared` crece sin control, acumulando l√≥gica que no es verdaderamente compartida. Con el tiempo, se convierte en un "mini-monolito" y el punto de mayor acoplamiento del sistema, traicionando el prop√≥sito de VSA.
* **S√≠ntomas**:
  * Casi cualquier cambio en un feature requiere modificar c√≥digo en `shared`.
  * La carpeta `shared` contiene l√≥gica de negocio compleja en lugar de solo componentes de infraestructura y modelos de dominio estables.
  * Los desarrolladores a√±aden c√≥digo a `shared` "por si acaso" se necesita en otro lugar.
* **Soluci√≥n / Mitigaci√≥n**:
  * **Adherirse a la "Regla de Tres"**: Un fragmento de c√≥digo solo se mueve a `shared` cuando es necesitado por una **tercera** feature. Con dos, la duplicaci√≥n suele ser preferible a una abstracci√≥n prematura y err√≥nea.
  * **Auditor√≠as de C√≥digo Frecuentes**: Las revisiones de c√≥digo deben cuestionar expl√≠citamente cualquier adici√≥n a `shared`. Preguntar: "¬øEs esto infraestructura agn√≥stica al dominio, un modelo de dominio central o una utilidad pura?".
  * **`shared` no debe contener l√≥gica de casos de uso**: Si un c√≥digo orquesta un proceso de negocio, pertenece a un *feature*, no a `shared`.

### 2\. Transacciones entre Features

* **Problema**: Un requerimiento de negocio exige que una operaci√≥n at√≥mica afecte a datos que son propiedad de dos *slices* diferentes. Por ejemplo, "al subir un artefacto (`UploadSlice`), se debe actualizar la cuota de uso del repositorio (`RepoManagementSlice`)".
* **S√≠ntomas**:
  * Un `handler` de un feature intenta llamar directamente a la l√≥gica de otro feature para "completar la transacci√≥n". Esto viola la regla de dependencia.
  * Se intentan implementar transacciones distribuidas (Two-Phase Commit) dentro del monolito, a√±adiendo una complejidad enorme.
* **Soluci√≥n / Mitigaci√≥n**:
  * **Consistencia Eventual (Patr√≥n Saga)**: Es el enfoque preferido.
    1. El `UploadSlice` hace su trabajo y guarda el artefacto. Al final, emite un evento `ArtifactUploaded`.
    2. El `RepoManagementSlice` se suscribe a ese evento. Al recibirlo, actualiza la cuota de uso en su propia transacci√≥n.
  * **Manejo de Fallos**: El sistema debe ser resiliente. Si el consumidor del evento falla, Kafka garantiza que el evento puede ser reintentado. Se deben implementar pol√≠ticas de reintento y colas de "mensajes fallidos" (dead-letter queues) para an√°lisis manual.
  * **Aceptar la Consistencia Eventual**: El negocio debe entender que la cuota no se actualizar√° en el mismo nanosegundo que la subida, sino milisegundos despu√©s. Para la gran mayor√≠a de los casos, esto es perfectamente aceptable.

### 3\. Consultas y Reporting Transversales

* **Problema**: Se necesita generar un informe que combine datos de m√∫ltiples *slices*. Por ejemplo, "mostrar todos los artefactos subidos por usuarios de un departamento espec√≠fico", donde los artefactos son de `UploadSlice` y los datos de los usuarios de `UserManagementSlice`.
* **S√≠ntomas**:
  * Un *feature* intenta consultar directamente las tablas de la base de datos de otro *feature*.
  * Se crean "endpoints de sincronizaci√≥n" complejos solo para obtener datos para un informe.
* **Soluci√≥n / Mitigaci√≥n**:
  * **Composici√≥n en la API Gateway o BFF (Backend-For-Frontend)**: Para consultas en tiempo real, el cliente puede realizar dos llamadas separadas (una a cada *slice*) y combinar los datos. O, preferiblemente, un servicio ligero como un BFF puede hacer estas dos llamadas en el backend y presentar un resultado unificado.
  * **Data Warehouse / Read Model Denormalizado**: Para reporting complejo y anal√≠tica, la mejor soluci√≥n es que cada *slice* publique eventos con sus cambios (`UserUpdated`, `ArtifactUploaded`). Un proceso ETL separado consume estos eventos y los proyecta en una base de datos optimizada para lectura (un "read model" o un data warehouse), donde los datos ya est√°n denormalizados y listos para ser consultados.

### 4\. Disciplina del Equipo y Consistencia

* **Problema**: La arquitectura VSA no es reforzada por el compilador de forma tan estricta como otras reglas. Requiere que el equipo entienda y respete los l√≠mites de los *slices*.
* **S√≠ntomas**:
  * Aparici√≥n de dependencias directas `use crate::features::otro_feature;` dentro de un *slice*.
  * C√≥digo de infraestructura (ej. llamadas a la base de datos) mezclado dentro de `logic.rs` en lugar de estar tras una abstracci√≥n.
* **Soluci√≥n / Mitigaci√≥n**:
  * **Documentaci√≥n Viva**: Este documento debe ser el punto de referencia.
  * **Plantillas de C√≥digo (Scaffolding)**: Crear herramientas o scripts (`cargo-generate`) que generen la estructura de carpetas y archivos para una nueva feature. Esto asegura que todos los nuevos *slices* partan de una base consistente y correcta.
  * **Ownership y Revisiones de C√≥digo**: Designar "due√±os" o expertos en ciertas √°reas del c√≥digo que presten especial atenci√≥n a mantener la integridad arquitect√≥nica durante las revisiones de c√≥digo. La pregunta clave siempre es: "¬øPertenece este c√≥digo a este *slice*?".
